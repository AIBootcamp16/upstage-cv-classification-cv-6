# main.py
# ë¬¸ì„œ ë¶„ë¥˜ - ë¡œì»¬ ì‹¤í–‰ìš© (ë°ì´í„° ìë™ ë‹¤ìš´ë¡œë“œ í¬í•¨)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import timm
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score
from sklearn.utils.class_weight import compute_class_weight
import os
from datetime import datetime
from pathlib import Path
from PIL import Image
import random
import subprocess
import urllib.request

# ========== ë°ì´í„° ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ ==========
def download_and_extract_data(data_dir='data'):
    """ë°ì´í„° ìë™ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ"""
    data_path = Path(data_dir)
    
    if data_path.exists() and (data_path / 'train.csv').exists():
        print("âœ… ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œ ê±´ë„ˆëœ€.\n")
        return
    
    print("="*70)
    print("ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
    print("="*70)
    
    DATA_URL = "https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000372/data/data.tar.gz"
    DATA_FILE = "data.tar.gz"
    
    try:
        # ë‹¤ìš´ë¡œë“œ
        print(f"ğŸŒ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {DATA_URL}")
        urllib.request.urlretrieve(DATA_URL, DATA_FILE)
        print("âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        
        # ì••ì¶• í•´ì œ
        print("\nğŸ“¦ ì••ì¶• í•´ì œ ì¤‘...")
        import tarfile
        with tarfile.open(DATA_FILE, 'r:gz') as tar:
            tar.extractall('.')
        print("âœ… ì••ì¶• í•´ì œ ì™„ë£Œ")
        
        # ì••ì¶• íŒŒì¼ ì‚­ì œ
        os.remove(DATA_FILE)
        print("ğŸ—‘ï¸  ì••ì¶• íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
        
        # ë°ì´í„° êµ¬ì¡° í™•ì¸
        print("\nğŸ“ ë°ì´í„° êµ¬ì¡°:")
        if data_path.exists():
            for item in data_path.iterdir():
                print(f"  - {item.name}")
        
        print("\nâœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
        print("="*70 + "\n")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ìˆ˜ë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ 'data/' í´ë”ì— ë°°ì¹˜í•´ì£¼ì„¸ìš”.")
        raise

# ========== Augraphy ì²´í¬ ==========
try:
    from augraphy import InkBleed, PaperFactory, DirtyDrum, Jpeg, Brightness, AugraphyPipeline
    AUGRAPHY_AVAILABLE = True
except ImportError:
    AUGRAPHY_AVAILABLE = False
    print("âš ï¸  Augraphy not installed. Using Albumentations only.")
    print("   Install with: pip install augraphy\n")

# ========== ì„¤ì • ==========
class Config:
    """í•™ìŠµ ì„¤ì • í´ë˜ìŠ¤"""
    def __init__(self):
        # ë°ì´í„° ê²½ë¡œ (ë¡œì»¬)
        self.DATA_DIR = 'data'
        self.TRAIN_DIR = 'data/train'
        self.TEST_DIR = 'data/test'
        
        # ëª¨ë¸ ì„¤ì •
        self.MODEL_NAME = 'tf_efficientnetv2_s'  # ë˜ëŠ” 'tf_efficientnetv2_m', 'convnext_tiny'
        self.IMG_SIZE = 384
        self.NUM_CLASSES = 17
        
        # í•™ìŠµ ì„¤ì •
        self.BATCH_SIZE = 8
        self.ACCUMULATION_STEPS = 2  # íš¨ê³¼ì  ë°°ì¹˜ = 8 * 2 = 16
        self.EPOCHS = 15
        self.LR = 0.0001
        self.N_FOLDS = 5
        
        # ì •ê·œí™” (ê³¼ì í•© ë°©ì§€)
        self.DROPOUT_RATE = 0.4
        self.WEIGHT_DECAY = 0.01
        self.LABEL_SMOOTHING = 0.05
        self.PATIENCE = 3
        
        # ì¦ê°• ì„¤ì •
        self.AUG_STRATEGY = 'hybrid'  # 'albumentations', 'augraphy', 'hybrid'
        self.AUGRAPHY_STRENGTH = 'light'  # 'light', 'medium', 'heavy'
        
        # ê¸°íƒ€
        self.USE_MIXUP = False
        self.MIXUP_ALPHA = 0.2
        self.USE_CLASS_WEIGHTS = True
        self.SEED = 42
        self.DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'
        
    def print_config(self):
        """ì„¤ì • ì¶œë ¥"""
        print('='*70)
        print('âš™ï¸  ì‹¤í—˜ ì„¤ì •')
        print('='*70)
        print(f'ëª¨ë¸: {self.MODEL_NAME}')
        print(f'ì´ë¯¸ì§€ í¬ê¸°: {self.IMG_SIZE}')
        print(f'ë°°ì¹˜ í¬ê¸°: {self.BATCH_SIZE} (íš¨ê³¼ì : {self.BATCH_SIZE * self.ACCUMULATION_STEPS})')
        print(f'ì—í­: {self.EPOCHS}, í•™ìŠµë¥ : {self.LR}')
        print(f'Fold ìˆ˜: {self.N_FOLDS}, Patience: {self.PATIENCE}')
        print(f'Dropout: {self.DROPOUT_RATE}, Weight Decay: {self.WEIGHT_DECAY}')
        print(f'ì¦ê°• ì „ëµ: {self.AUG_STRATEGY}')
        if self.AUG_STRATEGY in ['augraphy', 'hybrid']:
            print(f'Augraphy ê°•ë„: {self.AUGRAPHY_STRENGTH}')
        print(f'ë””ë°”ì´ìŠ¤: {self.DEVICE}')
        print('='*70)

# ê¸€ë¡œë²Œ ì„¤ì •
config = Config()
TIMESTAMP = None

# ========== Seed ì„¤ì • ==========
def set_seed(seed=42):
    """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    if torch.backends.mps.is_available():
        torch.mps.manual_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

# ========== ì¦ê°• í•¨ìˆ˜ë“¤ ==========
def get_albumentations_train(image_size):
    """ì¼ë°˜ ì´ë¯¸ì§€ìš© ì¦ê°•"""
    return A.Compose([
        A.Resize(image_size, image_size),
        A.Affine(translate_percent=0.03, scale=(0.95, 1.05), rotate=(-3, 3), p=0.3),
        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),
        A.GaussNoise(p=0.2),
        A.OneOf([
            A.GaussianBlur(blur_limit=(3, 5), p=1.0),
            A.MotionBlur(blur_limit=5, p=1.0),
        ], p=0.2),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ])

def get_augraphy_train(image_size):
    """ë¬¸ì„œ íŠ¹í™” ì¦ê°•"""
    if not AUGRAPHY_AVAILABLE:
        return get_albumentations_train(image_size)
    
    ink_phase = [InkBleed(intensity_range=(0.1, 0.3), p=0.2)]
    paper_phase = [PaperFactory(p=0.2), DirtyDrum(p=0.1)]
    post_phase = [Jpeg(quality_range=(60, 95), p=0.2), Brightness(brightness_range=(0.95, 1.05), p=0.2)]
    augraphy_pipeline = AugraphyPipeline(ink_phase, paper_phase, post_phase)
    
    def apply_augraphy_safe(image, **kwargs):
        result = augraphy_pipeline.augment(image)["output"]
        if len(result.shape) == 2:
            result = np.stack([result] * 3, axis=-1)
        elif result.shape[-1] == 1:
            result = np.repeat(result, 3, axis=-1)
        elif result.shape[-1] == 4:
            result = result[:, :, :3]
        return result
    
    return A.Compose([
        A.Lambda(image=apply_augraphy_safe),
        A.Resize(image_size, image_size),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

def get_hybrid_train(image_size, augraphy_strength='light'):
    """Augraphy + Albumentations í˜¼í•©"""
    if not AUGRAPHY_AVAILABLE:
        return get_albumentations_train(image_size)
    
    if augraphy_strength == 'light':
        ink_p, paper_p, post_p = 0.2, 0.2, 0.2
    elif augraphy_strength == 'medium':
        ink_p, paper_p, post_p = 0.4, 0.4, 0.3
    else:
        ink_p, paper_p, post_p = 0.6, 0.5, 0.4
    
    ink_phase = [InkBleed(intensity_range=(0.05, 0.15), p=ink_p)]
    paper_phase = [PaperFactory(p=paper_p), DirtyDrum(p=paper_p * 0.5)]
    post_phase = [Jpeg(quality_range=(70, 95), p=post_p), Brightness(brightness_range=(0.95, 1.05), p=post_p)]
    augraphy_pipeline = AugraphyPipeline(ink_phase, paper_phase, post_phase)
    
    def apply_augraphy_safe(image, **kwargs):
        result = augraphy_pipeline.augment(image)["output"]
        if len(result.shape) == 2:
            result = np.stack([result] * 3, axis=-1)
        elif result.shape[-1] == 1:
            result = np.repeat(result, 3, axis=-1)
        elif result.shape[-1] == 4:
            result = result[:, :, :3]
        return result
    
    return A.Compose([
        A.Lambda(image=apply_augraphy_safe),
        A.Rotate(limit=3, p=0.4),
        A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.4),
        A.GaussNoise(p=0.2),
        A.Resize(image_size, image_size),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

def get_val_transform(image_size):
    """ê²€ì¦ìš© ë³€í™˜"""
    return A.Compose([
        A.Resize(image_size, image_size),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ])

def get_train_transform(cfg):
    """Config ê¸°ë°˜ ì¦ê°• ì„ íƒ"""
    if cfg.AUG_STRATEGY == 'albumentations':
        return get_albumentations_train(cfg.IMG_SIZE)
    elif cfg.AUG_STRATEGY == 'augraphy':
        return get_augraphy_train(cfg.IMG_SIZE)
    elif cfg.AUG_STRATEGY == 'hybrid':
        return get_hybrid_train(cfg.IMG_SIZE, cfg.AUGRAPHY_STRENGTH)
    else:
        return get_albumentations_train(cfg.IMG_SIZE)

# ========== ë°ì´í„°ì…‹ ==========
class DocumentDataset(Dataset):
    def __init__(self, df, img_dir, transform=None, is_test=False):
        self.df = df.reset_index(drop=True)
        self.img_dir = Path(img_dir)
        self.transform = transform
        self.is_test = is_test
        
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = self.img_dir / row['ID']
        
        try:
            image = Image.open(img_path).convert('RGB')
            image = np.array(image)
        except Exception as e:
            print(f"âš ï¸  Error loading {img_path}: {e}")
            image = np.zeros((224, 224, 3), dtype=np.uint8)
        
        if self.transform:
            image = self.transform(image=image)['image']
        
        if self.is_test:
            return image
        else:
            label = row['label']
            return image, label

# ========== í•™ìŠµ í•¨ìˆ˜ ==========
def train_epoch(model, loader, criterion, optimizer, scheduler, cfg):
    model.train()
    losses = []
    optimizer.zero_grad()
    
    for idx, (images, labels) in enumerate(tqdm(loader, desc='Train')):
        images = images.to(cfg.DEVICE).contiguous()
        labels = labels.to(cfg.DEVICE)
        
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        loss = loss / cfg.ACCUMULATION_STEPS
        loss.backward()
        
        if (idx + 1) % cfg.ACCUMULATION_STEPS == 0 or (idx + 1) == len(loader):
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            optimizer.zero_grad()
        
        losses.append(loss.item() * cfg.ACCUMULATION_STEPS)
    
    scheduler.step()
    return np.mean(losses)

def validate(model, loader, cfg):
    model.eval()
    preds_list = []
    labels_list = []
    
    with torch.no_grad():
        for images, labels in tqdm(loader, desc='Val'):
            images = images.to(cfg.DEVICE)
            outputs = model(images)
            preds = outputs.argmax(dim=1)
            
            preds_list.extend(preds.cpu().numpy())
            labels_list.extend(labels.numpy())
    
    f1 = f1_score(labels_list, preds_list, average='macro')
    return f1

# ========== í´ë“œ í•™ìŠµ ==========
def train_fold(fold, train_df, val_df, exp_dir, class_weights, cfg):
    print(f'\n{"="*50}')
    print(f'Fold {fold} í•™ìŠµ ì‹œì‘')
    print(f'{"="*50}')
    
    train_transform = get_train_transform(cfg)
    val_transform = get_val_transform(cfg.IMG_SIZE)
    
    train_dataset = DocumentDataset(train_df, cfg.TRAIN_DIR, train_transform)
    val_dataset = DocumentDataset(val_df, cfg.TRAIN_DIR, val_transform)
    
    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=4)
    
    model = timm.create_model(cfg.MODEL_NAME, pretrained=True, num_classes=cfg.NUM_CLASSES, drop_rate=cfg.DROPOUT_RATE)
    model = model.to(cfg.DEVICE)
    print(f'âœ… ëª¨ë¸ ë¡œë“œ: {cfg.MODEL_NAME}')
    
    optimizer = optim.AdamW(model.parameters(), lr=cfg.LR, weight_decay=cfg.WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=1e-6)
    
    if cfg.USE_CLASS_WEIGHTS and class_weights is not None:
        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=cfg.LABEL_SMOOTHING)
    else:
        criterion = nn.CrossEntropyLoss(label_smoothing=cfg.LABEL_SMOOTHING)
    
    best_f1 = 0
    best_model_state = None
    patience_counter = 0
    
    for epoch in range(cfg.EPOCHS):
        train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler, cfg)
        val_f1 = validate(model, val_loader, cfg)
        
        current_lr = optimizer.param_groups[0]['lr']
        print(f'Epoch {epoch+1}/{cfg.EPOCHS} - Loss: {train_loss:.4f}, F1: {val_f1:.4f}, LR: {current_lr:.6f}')
        
        if val_f1 > best_f1:
            best_f1 = val_f1
            best_model_state = model.state_dict().copy()
            patience_counter = 0
            print(f'âœ… Best F1: {best_f1:.4f}')
        else:
            patience_counter += 1
            print(f'â³ Patience: {patience_counter}/{cfg.PATIENCE}')
        
        if patience_counter >= cfg.PATIENCE:
            print(f'Early stopping at epoch {epoch+1}')
            break
    
    model_filename = f'{exp_dir}/models/fold{fold}_{TIMESTAMP}_f1{best_f1:.4f}.pth'
    torch.save(best_model_state, model_filename)
    print(f'\nFold {fold} ì™„ë£Œ - Best F1: {best_f1:.4f}')
    
    return best_f1, model_filename

# ========== ì•™ìƒë¸” ì¶”ë¡  ==========
def inference_ensemble(test_df, fold_info, cfg):
    print(f'\n{"="*50}')
    print(f'ì¶”ë¡  ì‹œì‘ (ëª¨ë¸ {len(fold_info)}ê°œ)')
    print(f'{"="*50}')
    
    test_transform = get_val_transform(cfg.IMG_SIZE)
    test_dataset = DocumentDataset(test_df, cfg.TEST_DIR, test_transform, is_test=True)
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)
    
    models = []
    fold_f1s = []
    
    for fold, f1, model_path in fold_info:
        model = timm.create_model(cfg.MODEL_NAME, pretrained=False, num_classes=cfg.NUM_CLASSES)
        model.load_state_dict(torch.load(model_path, weights_only=False))
        model = model.to(cfg.DEVICE)
        model.eval()
        models.append(model)
        fold_f1s.append(f1)
        print(f'âœ… Fold {fold} (F1: {f1:.4f}) ë¡œë“œ')
    
    avg_f1 = np.mean(fold_f1s)
    weights = torch.tensor(fold_f1s, dtype=torch.float32)
    weights = weights / weights.sum()
    
    all_predictions = []
    
    for images in tqdm(test_loader, desc='Inference'):
        images = images.to(cfg.DEVICE)
        
        fold_preds = []
        for model in models:
            with torch.no_grad():
                pred = model(images)
            fold_preds.append(pred.cpu())
        
        fold_preds_tensor = torch.stack(fold_preds)
        weights_expanded = weights.unsqueeze(1).unsqueeze(2)
        ensemble_pred = (fold_preds_tensor * weights_expanded).sum(dim=0)
        final_class = ensemble_pred.argmax(dim=1).item()
        all_predictions.append(final_class)
    
    return all_predictions, avg_f1

# ========== ì œì¶œ íŒŒì¼ ìƒì„± ==========
def create_submission(test_df, predictions, avg_f1, exp_dir):
    filename = f'{exp_dir}/submission_{TIMESTAMP}_f1{avg_f1:.4f}.csv'
    
    submission = pd.DataFrame({
        'ID': test_df['ID'],
        'target': predictions
    })
    
    submission.to_csv(filename, index=False)
    
    print(f'\n{"="*50}')
    print(f'ì œì¶œ íŒŒì¼: {filename}')
    print(f'{"="*50}')
    print(submission.head(10))
    print(f'\nì˜ˆì¸¡ ë¶„í¬:')
    print(submission['target'].value_counts().sort_index())
    
    return filename

# ========== ë©”ì¸ ì‹¤í–‰ ==========
if __name__ == '__main__':
    # ë°ì´í„° ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œ)
    download_and_extract_data()
    
    # ì´ˆê¸°í™”
    TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')
    set_seed(config.SEED)
    
    # ì‹¤í—˜ í´ë”
    EXP_DIR = f'experiments/exp_{TIMESTAMP}'
    os.makedirs(EXP_DIR, exist_ok=True)
    os.makedirs(f'{EXP_DIR}/models', exist_ok=True)
    
    print('\n'+'='*70)
    print('ğŸš€ ë¬¸ì„œ ë¶„ë¥˜ í•™ìŠµ ì‹œì‘')
    print('='*70)
    config.print_config()
    
    # ë°ì´í„° ë¡œë“œ
    print(f'\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...')
    train_df = pd.read_csv(f'{config.DATA_DIR}/train.csv')
    train_df['label'] = train_df['target']
    
    print(f'í•™ìŠµ ë°ì´í„°: {len(train_df)}ì¥')
    print(f'í´ë˜ìŠ¤: {train_df["label"].nunique()}ê°œ')
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜
    if config.USE_CLASS_WEIGHTS:
        class_weights = compute_class_weight(
            'balanced',
            classes=np.unique(train_df['label']),
            y=train_df['label']
        )
        class_weights = torch.FloatTensor(class_weights).to(config.DEVICE)
    else:
        class_weights = None
    
    # K-Fold í•™ìŠµ
    skf = StratifiedKFold(n_splits=config.N_FOLDS, shuffle=True, random_state=42)
    fold_results = []
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['label']), start=1):
        train_fold_df = train_df.iloc[train_idx]
        val_fold_df = train_df.iloc[val_idx]
        
        best_f1, model_path = train_fold(fold, train_fold_df, val_fold_df, EXP_DIR, class_weights, config)
        
        fold_results.append({
            'fold': fold,
            'f1': best_f1,
            'model_path': model_path
        })
    
    # ê²°ê³¼ ì¶œë ¥
    results_df = pd.DataFrame(fold_results)
    print(f'\n{"="*50}')
    print('ğŸ“Š í•™ìŠµ ê²°ê³¼')
    print(f'{"="*50}')
    print(results_df[['fold', 'f1']])
    print(f'\ní‰ê·  F1: {results_df["f1"].mean():.4f}')
    print(f'ìµœê³  F1: {results_df["f1"].max():.4f}')
    
    # ê²°ê³¼ ì €ì¥
    results_filename = f'{EXP_DIR}/fold_results_{TIMESTAMP}.csv'
    results_df.to_csv(results_filename, index=False)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    if os.path.exists(f'{config.DATA_DIR}/test.csv'):
        test_df = pd.read_csv(f'{config.DATA_DIR}/test.csv')
    else:
        test_df = pd.read_csv(f'{config.DATA_DIR}/sample_submission.csv')
        test_df = test_df.drop('target', axis=1)
    
    print(f'\ní…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ì¥')
    
    # ì•™ìƒë¸” ì¶”ë¡ 
    fold_info = [
        (row['fold'], row['f1'], row['model_path'])
        for _, row in results_df.iterrows()
    ]
    
    predictions, avg_f1 = inference_ensemble(test_df, fold_info, config)
    
    # ì œì¶œ íŒŒì¼ ìƒì„±
    submission_filename = create_submission(test_df, predictions, avg_f1, EXP_DIR)
    
    print(f'\n{"="*70}')
    print('âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!')
    print(f'{"="*70}')
    print(f'ğŸ“ ê²°ê³¼ íŒŒì¼: {results_filename}')
    print(f'ğŸ“ ì œì¶œ íŒŒì¼: {submission_filename}')
    print(f'{"="*70}\n')